{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9481f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb3c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "display(Image(filename=\"images/intro-image.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6c183",
   "metadata": {},
   "source": [
    "## Highlights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586273fc",
   "metadata": {},
   "source": [
    "- Monitor workstations to ensure that no workstation remains idle, and ensure that all workstations are utilized at 100% capacity.\n",
    "- Implement employee monitoring to ensure the equitable tracking of hours worked by employees.\n",
    "- Utilizing YOLO (You Only Look Once) for the purpose of both counting and detecting individuals in a room.\n",
    "- The process of fine-tuning YOLO on a custom dataset to enhance the accuracy of the results.\n",
    "- Utilizing Intersection over Union (IoU) as a threshold to determine whether a station is taken or vacant.\n",
    "- Applying image processing techniques and using Pytesseract to perform Optical Character Recognition (OCR) in order to extract the timestamp from the CCTV camera feed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a6d03a",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de349404",
   "metadata": {},
   "source": [
    "The solution that I implemented in this project is to use deep learning techniques to detect an employee and monitor the time the employee is seated in its station. An additional feature in this project is to count the number of person inside the office. To be able to track if there are some violations on social distancing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391177b",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3331c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python standard libraries\n",
    "import os, time, glob, re, shutil, pickle\n",
    "from datetime import datetime, timedelta\n",
    "from base64 import b64decode, b64encode\n",
    "\n",
    "# google colab/notebook libraries\n",
    "from IPython.display import display, Javascript, Image\n",
    "from IPython.display import Video\n",
    "from js2py import eval_js\n",
    "\n",
    "# external libraries\n",
    "import cv2, PIL, io, html, pytesseract\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from skimage.morphology import erosion\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e297277",
   "metadata": {},
   "source": [
    "## Define Color Constants For Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d4f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_color = (220, 155, 58)\n",
    "vacant_color = (0, 0, 200)\n",
    "taken_color = (0, 200, 0)\n",
    "station_color = (0, 100, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec50fb1",
   "metadata": {},
   "source": [
    "## Define Workstation & DateTime Area Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = {\n",
    "    'station_1' : {'x1':1123, 'x2':1200, \n",
    "                   'y1':112, 'y2':176},\n",
    "    'station_2' : {'x1':1267, 'x2':1330, \n",
    "                   'y1':121, 'y2':195},               \n",
    "    'station_3' : {'x1':1378, 'x2':1450, \n",
    "                   'y1':138, 'y2':207},\n",
    "    'station_4' : {'x1':1483, 'x2':1553, \n",
    "                   'y1':166, 'y2':253},\n",
    "    'station_5' : {'x1':1608, 'x2':1680, \n",
    "                   'y1':204, 'y2':280},\n",
    "    'station_6' : {'x1':798, 'x2':872, \n",
    "                   'y1':148, 'y2':237},\n",
    "    'station_7' : {'x1':968, 'x2':1060, \n",
    "                   'y1':165, 'y2':269},\n",
    "    'station_8' : {'x1':1099, 'x2':1209, \n",
    "                   'y1':199, 'y2':336},\n",
    "    'station_9' : {'x1':1289, 'x2':1391, \n",
    "                   'y1':235, 'y2':366},\n",
    "    'station_10' : {'x1':374, 'x2':457, \n",
    "                   'y1':251, 'y2':355},\n",
    "    'station_11' : {'x1':471, 'x2':562, \n",
    "                   'y1':278, 'y2':481},\n",
    "    'station_12' : {'x1':575, 'x2':717, \n",
    "                   'y1':337, 'y2':557},\n",
    "    'station_13' : {'x1':727, 'x2':931, \n",
    "                   'y1':411, 'y2':684},\n",
    "    'station_14' : {'x1':964, 'x2':1194, \n",
    "                   'y1':530, 'y2':785}\n",
    "}\n",
    "\n",
    "coordinates_ocr= [(55, 43), (799, 91)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab593af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from IPython.display import Image\n",
    "\n",
    "def hide_cell():\n",
    "    HTML('''<script>\n",
    "    code_show=true; \n",
    "    function code_toggle() {\n",
    "     if (code_show){\n",
    "     $('div.input').hide();\n",
    "     } else {\n",
    "     $('div.input').show();\n",
    "     }\n",
    "     code_show = !code_show\n",
    "    } \n",
    "    $( document ).ready(code_toggle);\n",
    "    </script>\n",
    "    <style>\n",
    "    .output_png {\n",
    "        display: table-cell;\n",
    "        text-align: center;\n",
    "        horizontal-align: middle;\n",
    "        vertical-align: middle;\n",
    "        margin:auto;\n",
    "    }\n",
    "\n",
    "    tbody, thead {\n",
    "        margin-left:100px;\n",
    "    }\n",
    "\n",
    "    </style>\n",
    "    <form action=\"javascript:code_toggle()\"><input type=\"submit\"\n",
    "    value=\"Click here to toggle on/off the raw code.\"></form>''')\n",
    "# hide_cell()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8500e9",
   "metadata": {},
   "source": [
    "## Get Current Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current Working Directory: \\t\",os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce3ebe",
   "metadata": {},
   "source": [
    "## Change Makefile To Have GPU, OPENCV and LIBSO Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a747be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd darknet\n",
    "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n",
    "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
    "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
    "!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile\n",
    "!sed -i 's/LIBSO=0/LIBSO=1/' Makefile\n",
    "\n",
    "# make darknet (builds darknet so that you can then use the darknet.py file \n",
    "# and have its dependencies)\n",
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4278d1d",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78434923",
   "metadata": {},
   "source": [
    "Since it is just a single camera each of the frames would have the same size and channels. For exploratory data analysis, let us check the dimesions of one frame of the CCTV feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print sample image \n",
    "vidcap = cv2.VideoCapture('../cctv_footage/1.mp4')\n",
    "success, frame = vidcap.read()\n",
    "\n",
    "# Get the width and height of the frame\n",
    "frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "# Get Frame Per Second Information\n",
    "fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Get Total Frame From a video footage\n",
    "total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Print the width and height\n",
    "print(\"Frame Width:\", frame_width)\n",
    "print(\"Frame Height:\", frame_height)\n",
    "print(\"FPS:\", fps)\n",
    "print(\"Total Frame: \",total_frames)\n",
    "\n",
    "if success:\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')  # Optional: Turn off the axis if you don't want it\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d73c4c",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddbfa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=\"../images/methodology.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f5171",
   "metadata": {},
   "source": [
    "<b>a.) Dataset</b>\n",
    "<p style=\"text-align:justify\">The dataset employed for this project consists of personal video surveillance footage from our business, totaling 15 minutes in length, recorded in November 2023.</p>\n",
    "\n",
    "<b>b.) Extract images</b>\n",
    "<p style=\"text-align:justify\">Extracted 170 sample images to be used for training and validation set.</p>\n",
    "\n",
    "<b>c.) Fine tune YOLOv4 model</b>\n",
    "<p style=\"text-align:justify\">Conducted transfer learning by initially utilizing pretrained weights trained on the COCO dataset and then fine-tuned the model using our custom dataset.</p>\n",
    "\n",
    "<b>d.) Perform non-max suppression</b>\n",
    "<p style=\"text-align:justify\">Applied non-maximum suppression to eliminate multiple bounding boxes around a single object, retaining the one with the highest confidence score when there is an overlap.</p>\n",
    "\n",
    "<b>e.) Set-up work station</b>\n",
    "<p style=\"text-align:justify\">Established a workstation setup to establish and implement a rule based on the Intersection over Union (IoU) of the detected persons and the workstations, enabling the identification of which stations are occupied and which are vacant.</p>\n",
    "\n",
    "<b>f.) Compute for the time of the employee in work station</b>\n",
    "<p style=\"text-align:justify\">Employed the information provided by the DVR, located in the upper right of the image, and applied image processing techniques and Optical Character Recognition (OCR) to extract and convert the text into a datetime Python object..</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fe461",
   "metadata": {},
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b810379",
   "metadata": {},
   "source": [
    "<h2>1. Pre trained YOLO on COCO dataset</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bb4b0a",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">YOLO has pre trained model on COCO dataset which could classify <a href=\"https://gist.github.com/AruniRC/7b3dadd004da04c80198557db5da4bda\">80 objects</a>. <a href=\"https://cocodataset.org/#home\">COCO</a> dataset is a large scale object detection, segmentation, and captioning of over 330k images. One of the objects that the YOLO trained on this dataset could classify is person which is what we need for this study. Let us try to use if it would work well on our dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b435a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use if you want to use default settings\n",
    "# get the scaled yolov4 weights file that is pre-trained to detect 80 classes (objects) from shared google drive\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V3vsIaxAlGWvK4Aar9bAiK5U0QFttKwq' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1V3vsIaxAlGWvK4Aar9bAiK5U0QFttKwq\" -O yolov4-csp.weights && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acd7ebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import darknet functions to perform object detections\n",
    "from darknet import *\n",
    "# load in our YOLOv4 architecture network\n",
    "network, class_names, class_colors = load_network(\"cfg/yolov4-csp.cfg\", \"cfg/coco.data\", \"yolov4-csp.weights\")\n",
    "width = network_width(network)\n",
    "height = network_height(network)\n",
    "\n",
    "# darknet helper function to run detection on image\n",
    "def darknet_helper(img, width, height):\n",
    "    darknet_image = make_image(width, height, 3)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img_rgb, (width, height),\n",
    "                              interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # get image ratios to convert bounding boxes to proper size\n",
    "    img_height, img_width, _ = img.shape\n",
    "    width_ratio = img_width/width\n",
    "    height_ratio = img_height/height\n",
    "\n",
    "    # run model on darknet style image to get detections\n",
    "    copy_image_from_bytes(darknet_image, img_resized.tobytes())\n",
    "    detections = detect_image(network, class_names, darknet_image)\n",
    "    free_image(darknet_image)\n",
    "    return detections, width_ratio, height_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abb8745",
   "metadata": {},
   "source": [
    "## Evaluate In Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a3a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./darknet detector test cfg/coco.data cfg/yolov4-csp.cfg yolov4-csp.weights data/0.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fd94ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !./darknet detector demo cfg/coco.data cfg/yolov4-csp.cfg yolov4-csp.weights ../cctv_footage/Desire_Clip_02.mp4 -dont_show -ext_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff5d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=\"predictions.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3449ee4d",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">It turns out that for some of the frames it would perfectly classify the person in the image. However, there are some frames that the model misclassify the person. Additionally, there are sometimes multiple bounding boxes on a single object and some objects that are not needed in this project are being classified like chair, monitor, laptop etc. Although the model is trained on thousand of images, it was not really trained on this type of environment and probably not to all angles of a  person. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde13b46",
   "metadata": {},
   "source": [
    "## 2. Train YOLO on custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db09facd",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">To remedy the issues found in using pretrained model on COCO dataset, we can perform training on custom dataset. The detailed explanation on how to train your YOLO on custom dataset can be found in their <a href=\"https://github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects\">documentation</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080d530c",
   "metadata": {},
   "source": [
    "<h3>a. Extract custom dataset </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea55cb8c",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">Training YOLO on custom dataset would require images, so we need to transform our video and sample it into images. <a href=\"https://labelstud.io/blog/Quickly-Create-Datasets-for-Training-YOLO-Object-Detection.html\">50 to 100</a> images are usually enough to train a single object but for this project I sampled the data into 200 images to ensure more correctness of the data.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7595431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_frame():\n",
    "    cap = cv2.VideoCapture('../cctv_footage/1.mp4')\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        exit()\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    output_directory = 'frames1'\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    img_array = []\n",
    "    counter = 0\n",
    "    image_count = 171\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Break out of the loop when there are no more frames\n",
    "\n",
    "        if counter % int(length / 196) == 0:\n",
    "            fname = os.path.join(output_directory, f'{image_count}.jpg')\n",
    "            image_count += 1\n",
    "            cv2.imwrite(fname, frame)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "# create_image_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d8e13",
   "metadata": {},
   "source": [
    "<h3>b. Manually label person in custom dataset </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec5e1ef",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">Now that we have our 170 images, we need to manually label the images in a format expected by the YOLO algorithm which is <code>{object-class} {x_center} {y_center} {width} {height}</code>. I used <a href=\"https://github.com/tzutalin/labelImg\">LabelImg</a> to manually label my data into bounding boxes. It will produce text files of bounding boxes with the same filename as the image file and a text file that contains the class names. We then split the data into train and 10 percent validation set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf81f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_labelling():\n",
    "    current_dir = './data/obj'\n",
    "\n",
    "    # Percentage of images to be used for the test set\n",
    "    percentage_test = 20;\n",
    "\n",
    "    # Create and/or truncate train.txt and test.txt\n",
    "    file_train = open('./data/train.txt', 'w')\n",
    "    file_test = open('./data/test.txt', 'w')\n",
    "\n",
    "    # Populate train.txt and test.txt\n",
    "    counter = 1\n",
    "    index_test = round(100 / percentage_test)\n",
    "    for pathAndFilename in glob.iglob(os.path.join(current_dir, \"*.jpg\")):\n",
    "        title, ext = os.path.splitext(os.path.basename(pathAndFilename))\n",
    "\n",
    "        if counter == index_test:\n",
    "            counter = 1\n",
    "            file_test.write(\"data/obj\" + \"/\" + title + '.jpg' + \"\\n\")\n",
    "        else:\n",
    "            file_train.write(\"data/obj\" + \"/\" + title + '.jpg' + \"\\n\")\n",
    "            counter = counter + 1\n",
    "            \n",
    "            \n",
    "# data_labelling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2dfc78",
   "metadata": {},
   "source": [
    "<h3>c. Train the model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8276cadb",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">The model training can be performed by using the <code>detector train</code> command which expects at least three parameters: data, configurations, and initial weights. Initially I used <code>yolov4.conv.137</code> as my pre trained weights which is trained in COCO dataset. Then, I retrained my weights to further improve the results. Overall it took me about six hours of training to get a good result.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea995b4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train by transfer learning from weights trained on coco dataset by darknet\n",
    "def train():\n",
    "#     !./darknet detector train data/obj.data cfg/yolov4-obj.cfg yolov4.conv.137 -dont_show -map\n",
    "    !./darknet detector train data/obj.data cfg/yolov4-obj.cfg backup/yolov4-obj_last.weights -dont_show\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e8ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue training model by transfer learning from weights trained on custom dataset\n",
    "def train_on_custom_set():\n",
    "    !./darknet detector train data/obj.data cfg/yolov4-obj.cfg backup/yolov4-obj_last.weights -dont_show -map\n",
    "# train_on_custom_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b2a22d",
   "metadata": {},
   "source": [
    "## Check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da82ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper function imShow\n",
    "def imShow(path):\n",
    "    image = cv2.imread(path)\n",
    "    height, width = image.shape[:2]\n",
    "    resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(18, 10)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "imShow('chart.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7f8ff1",
   "metadata": {},
   "source": [
    "## Check mAP (mean average precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673f822",
   "metadata": {},
   "source": [
    "You can check mAP for all the weights saved every 1000 iterations for eg:- yolov4-custom_4000.weights, yolov4-custom_5000.weights, yolov4-custom_6000.weights, and so on. This way you can find out which weights file gives you the best result. The higher the mAP the better it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52afdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./darknet detector map data/obj.data cfg/yolov4-obj.cfg backup/yolov4-obj_5000.weights -points 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd6105",
   "metadata": {},
   "source": [
    "## Test your custom Object Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f3303",
   "metadata": {},
   "source": [
    "Create a copy of yolov4=obj.cfg config file to set it to test mode\n",
    "\n",
    "Change line batch to batch=1\n",
    "\n",
    "Change line subdivisions to subdivisions=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21bf537",
   "metadata": {},
   "source": [
    "## Run detector on an image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37b4352",
   "metadata": {},
   "source": [
    "Run your custom detector on an image with this command. (The thresh flag sets the minimum accuracy required for object detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./darknet detector test data/obj.data cfg/yolov4-obj-test.cfg backup/yolov4-obj_final.weights data/360.jpg -thresh 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1333efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=\"predictions.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17ba1d0",
   "metadata": {},
   "source": [
    "## Run detector on a video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fcde56",
   "metadata": {},
   "source": [
    "Run your custom detector on a video with this command. (The thresh flag sets the minimum accuracy required for object detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./darknet detector demo data/obj.data cfg/yolov4-obj-test.cfg backup/yolov4-obj_best.weights -dont_show ../cctv_footage/Desire_Clip_02.mp4 -thresh 0.5 -i 0 -out_filename ../cctv_footage/results1.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74068cd7",
   "metadata": {},
   "source": [
    "<h3>d. Perform Non-Max Suppression</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21fb4c7",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">The non max supression created by <a href=\"https://github.com/AlexeyAB/darknet/blob/master/darknet.py\">darknet</a> prioritizes the bottom right bounding box and removes the overlap. I have updated the code of darknet to get the maximum confidence of the bounding boxes and remove the overlap. The solution could be slower than darknet implementation but it yielded into better accuracy which is more important in this project. I chose 65% threshold for the non-max suppression as it provided optimal result.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef547e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast1(detections, overlap_thresh):\n",
    "    \"\"\" modified non max suppression from darknet to get the overlap\n",
    "        with max confidence\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    detections       :     tuple\n",
    "                           class_name, confidence, and coordinates\n",
    "    overlap_thresh   :     float\n",
    "                           IOU threshold\n",
    "    \n",
    "    Returns\n",
    "    ==========\n",
    "    non_max_suppression_fast   :   tuple\n",
    "                                   detections without high overlap\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    confs = []\n",
    "\n",
    "    for detection in detections:\n",
    "        class_name, conf, (x, y, w, h) = detection\n",
    "#         print(\"\\n\\nclass_name\",class_name, \"\\n\\nconf\",conf, \"\\n\\nx\",x, \"\\n\\ny\",y, \"\\n\\nw\",w, \"\\n\\nh\",h)\n",
    "        x1 = x - w / 2\n",
    "        y1 = y - h / 2\n",
    "        x2 = x + w / 2\n",
    "        y2 = y + h / 2\n",
    "        boxes.append(np.array([x1, y1, x2, y2]))\n",
    "        confs.append(conf)\n",
    "   \n",
    "    boxes_array = np.array(boxes)\n",
    "\n",
    "    # initialize the list of picked indexes\n",
    "    pick = []\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes_array[:, 0]\n",
    "    y1 = boxes_array[:, 1]\n",
    "    x2 = boxes_array[:, 2]\n",
    "    y2 = boxes_array[:, 3]\n",
    "    # compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the bottom-right y-coordinate of the bounding box\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "    confs = np.array(confs)\n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the\n",
    "        # index value to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        # find the largest (x, y) coordinates for the start of\n",
    "        # the bounding box and the smallest (x, y) coordinates\n",
    "        # for the end of the bounding box\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "        # compute the width and height of the bounding box\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "\n",
    "        # choose the highest confidence among overlaps\n",
    "        overlap_args = np.where(overlap > overlap_thresh)[0]\n",
    "        overlap_indices = idxs[overlap_args].tolist() + [i]\n",
    "        confidence_list = confs[idxs[overlap_args]].tolist() + [confs[i]]\n",
    "        confidence_list = list(map(float, confidence_list))\n",
    "        highest_confidence = np.argmax(confidence_list)\n",
    "        pick.append(overlap_indices[highest_confidence])\n",
    "\n",
    "        # delete indices that overlaps\n",
    "        idxs = np.delete(idxs, np.concatenate(([last], overlap_args)))\n",
    "\n",
    "    return [detections[i] for i in pick]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8df605",
   "metadata": {},
   "source": [
    "<h3>e. Inference custom YOLO </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ffc0eb",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">Now that we have trained our custom YOLO model and created a custom non-max suppression, we can now try if the model would work on a test set. The average precision increased from 69% trained on coco dataset to 98% in our custom dataset. You can evaluate using mean average precision of the object. It can be calculated by adjusting the threshold of confidence and get the average precision score on 50% IoU score. It can be calculated by using <code>detector map</code> command of YOLOv4 repository.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec30810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import darknet functions to perform object detections\n",
    "from darknet import *\n",
    "# load in our YOLOv4 architecture network\n",
    "(network, \n",
    " class_names, \n",
    " class_colors) = load_network(\"cfg/yolov4-obj.cfg\", \n",
    "                              \"data/obj.data\", \n",
    "                              \"backup/yolov4-obj_final.weights\")\n",
    "width = network_width(network)\n",
    "height = network_height(network)\n",
    "\n",
    "# darknet helper function to run detection on image\n",
    "def darknet_helper(img, width, height):\n",
    "    \"\"\" darknet helper function to get detections, width and height ratio\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    img           :    np.array\n",
    "                       image file\n",
    "    width         :    int\n",
    "                       width\n",
    "    height        :    int\n",
    "                       height\n",
    "    \n",
    "    Returns\n",
    "    =========\n",
    "    darknet_helper  : tuple\n",
    "                      tuple of detections, width and height ratio\n",
    "    \"\"\"\n",
    "    darknet_image = make_image(width, height, 3)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img_rgb, (width, height),\n",
    "                              interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # get image ratios to convert bounding boxes to proper size\n",
    "    img_height, img_width, _ = img.shape\n",
    "    width_ratio = img_width/width\n",
    "    height_ratio = img_height/height\n",
    "\n",
    "    # run model on darknet style image to get detections\n",
    "    copy_image_from_bytes(darknet_image, img_resized.tobytes())\n",
    "    detections = detect_image(network, class_names, darknet_image)\n",
    "    free_image(darknet_image)\n",
    "    return detections, width_ratio, height_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8636851",
   "metadata": {},
   "source": [
    "<h3>f. Run Custom Yolo On sample Image  </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c7c827",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture('../cctv_footage/1.mp4')\n",
    "\n",
    "for i in range(15):\n",
    "    success,frame = vidcap.read()\n",
    "\n",
    "# get the predicted detections of the trained custom yolo\n",
    "detections, width_ratio, height_ratio = darknet_helper(frame, width, height)\n",
    "\n",
    "# apply non max suppression to eliminate multiple predictions\n",
    "# on same person\n",
    "detections = non_max_suppression_fast1(detections, 0.65)\n",
    "\n",
    "for label, confidence, bbox in detections:\n",
    "    left, top, right, bottom = bbox2points(bbox)\n",
    "    left, top, right, bottom = (int(left * width_ratio), int(top * height_ratio), \n",
    "    int(right * width_ratio), int(bottom * height_ratio))\n",
    "    cv2.rectangle(frame, (left, top), (right, bottom), person_color, 2)\n",
    "    cv2.putText(frame, \"{} [{:.2f}]\".format(label, float(confidence)),\n",
    "                    (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                    person_color, 2)\n",
    "\n",
    "# cv2_imshow(frame)\n",
    "\n",
    "# Check if the video frame was successfully read\n",
    "if success:\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')  # Optional: Turn off the axis if you don't want it\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6dd08",
   "metadata": {},
   "source": [
    "<h3>g. Evaluate In validation set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e6b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !./darknet detector map data/obj.data cfg/yolov4-obj.cfg backup/yolov4-obj_best.weights -points 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f677d3",
   "metadata": {},
   "source": [
    "<h2>3. Set up work station area</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c601c0ca",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">We now get the coordinates of the four work stations. I used paint to manually get the coordinates of each of the stations and plotted inte figure below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81452284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run custom yolo on a sample image \n",
    "# %cd ..\n",
    "vidcap = cv2.VideoCapture('../cctv_footage/1.mp4')\n",
    "success,frame = vidcap.read()\n",
    "\n",
    "\n",
    "for stations, coordinate in coordinates.items():\n",
    "    cv2.rectangle(frame, (coordinate['x1'], coordinate['y1']), \n",
    "                    (coordinate['x2'], coordinate['y2']), station_color, 2)\n",
    "    \n",
    "    text_size, baseline = cv2.getTextSize(f\" {stations}\", cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "                                   \n",
    "    cv2.rectangle(frame,\n",
    "              (coordinate['x1'], coordinate['y1'] - text_size[1] - 5),\n",
    "              (coordinate['x1'] + text_size[0], coordinate['y1'] - 5),\n",
    "              (128, 0, 128),\n",
    "              cv2.FILLED)\n",
    "    \n",
    "    cv2.putText(frame, f\" {stations}\",\n",
    "                (coordinate['x1'], coordinate['y1'] - 7), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "                (0,0,0), 2)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Optional: Turn off the axis if you don't want it\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba91fb",
   "metadata": {},
   "source": [
    "<h2>4. Integrate Custom YOLO on work stations</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9933b505",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">To integrate work stations in our custom network, we need to set up a rule to determine if the work station is taken or vacant. I used the Intersect Over Union (IoU) of 0.3 as a threshold if they overlap with the person to determine if the station is taken. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(bb1, bb2):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bb1 : dict\n",
    "        Keys: {'x1', 'x2', 'y1', 'y2'}\n",
    "        The (x1, y1) position is at the top left corner,\n",
    "        the (x2, y2) position is at the bottom right corner\n",
    "    bb2 : dict\n",
    "        Keys: {'x1', 'x2', 'y1', 'y2'}\n",
    "        The (x, y) position is at the top left corner,\n",
    "        the (x2, y2) position is at the bottom right corner\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        in [0, 1]\n",
    "    \"\"\"\n",
    "    # determine the coordinates of the intersection rectangle\n",
    "    x_left = max(bb1['x1'], bb2['x1'])\n",
    "    y_top = max(bb1['y1'], bb2['y1'])\n",
    "    x_right = min(bb1['x2'], bb2['x2'])\n",
    "    y_bottom = min(bb1['y2'], bb2['y2'])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    # The intersection of two axis-aligned bounding boxes is always an\n",
    "    # axis-aligned bounding box\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    # compute the area of both AABBs\n",
    "    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n",
    "    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    \n",
    "    return iou "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26481d67",
   "metadata": {},
   "source": [
    "### Run Custom Yolo On  Sample Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f1bf91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture('../cctv_footage/1.mp4')\n",
    "success,frame = vidcap.read()\n",
    "\n",
    "# get the predicted detections of the trained custom yolo\n",
    "detections, width_ratio, height_ratio = darknet_helper(frame, width, height)\n",
    "\n",
    "# apply non max suppression to eliminate multiple predictions\n",
    "# on same person\n",
    "detections = non_max_suppression_fast1(detections, 0.65)\n",
    "detections_bb = []\n",
    "for label, confidence, bbox in detections:\n",
    "    left, top, right, bottom = bbox2points(bbox)\n",
    "    left, top, right, bottom = (int(left * width_ratio), \n",
    "                                int(top * height_ratio), \n",
    "                                int(right * width_ratio), \n",
    "                                int(bottom * height_ratio))\n",
    "    \n",
    "    cv2.rectangle(frame, (left, top), (right, bottom), person_color, 2)\n",
    "    \n",
    "    text_size, _ = cv2.getTextSize(\"{} [{:.2f}]\".format(label, float(confidence)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "                                   \n",
    "    cv2.rectangle(frame,\n",
    "              (left, top - text_size[1] - 5),\n",
    "              (left + text_size[0], top - 5),\n",
    "              (0, 0, 0),  # Black color for the background\n",
    "              cv2.FILLED)\n",
    "    \n",
    "    \n",
    "    cv2.putText(frame, \"{} [{:.2f}]\".format(label, float(confidence)),\n",
    "                    (left, top - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "                    person_color, 2)\n",
    "\n",
    "    detections_bb.append({\n",
    "        'x1' : left,\n",
    "        'y1' : top,\n",
    "        'x2' : right,\n",
    "        'y2' : bottom\n",
    "    })\n",
    "\n",
    "thresh = 0.3\n",
    "for stations, coordinate in coordinates.items():\n",
    "    taken = False\n",
    "    for detection in detections_bb:\n",
    "        iou = get_iou(coordinate, detection)\n",
    "        if iou >= thresh:\n",
    "            taken = True\n",
    "            break\n",
    "    color = taken_color if taken else vacant_color\n",
    "        \n",
    "    cv2.rectangle(frame, (coordinate['x1'], coordinate['y1']), \n",
    "                    (coordinate['x2'], coordinate['y2']), color, 2)\n",
    "    \n",
    "    text_size, _ = cv2.getTextSize(f\"{stations}\", cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "        \n",
    "    cv2.rectangle(frame,\n",
    "          (coordinate['x1'], coordinate['y1'] - text_size[1] - 5),\n",
    "          (coordinate['x1'] + text_size[0], coordinate['y1'] - 5),\n",
    "          (0, 0, 0),  # Black color for the background\n",
    "          cv2.FILLED)\n",
    "    \n",
    "    cv2.putText(frame, f\"{stations}\",\n",
    "                (coordinate['x1'], coordinate['y1'] - 7), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "                color, 2)\n",
    "frame = cv2.resize(frame, (1080, 720), \n",
    "                    interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# cv2_imshow(frame)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Optional: Turn off the axis if you don't want it\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e83fb",
   "metadata": {},
   "source": [
    "<h2>5. Extract datetime information</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49434719",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">To get the time the employee stays in its work station, we can perform it in two ways: We can use the information of the image presented by the DVR which can be seen in the upper right of the image, or used the information of the camera by knowing the  frames per second of the camera. In this project I used both method but the former is more appropriate since based on my experience, CCTV cameras are often changed compared to the DVR. Cameras could have multiple frames per second, and using that information could yield to incorrect classification of time. Hence, extracting the information produced by the DVR would have longer usability of the model that we would create. Here is the sample of the original image:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf6aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture('../cctv_footage/Desire_Clip_02.mp4')\n",
    "success, frame = vidcap.read()\n",
    "if success:\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dbf407",
   "metadata": {},
   "source": [
    "## Mark Time/Date Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture('../cctv_footage/Desire_Clip_02.mp4')\n",
    "success,frame = vidcap.read()\n",
    "cv2.rectangle(frame, (coordinates_ocr[0][0], coordinates_ocr[0][1]), (coordinates_ocr[1][0], coordinates_ocr[1][1]), vacant_color, 2)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd6876",
   "metadata": {},
   "source": [
    "### a. Datetime information using OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027f5221",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">First, I performed image preprocessing in order for the OCR model to understand the text in the image more accurately. The image processing techniques performed are extract the image to get the area with the date time only, convert the image into BGR to Gray, adjust the contrast and brightness to make the background brighter before thresholding, perform adaptive thresholding to remove the background, and lasty use morphological operations such as multiple erosion to make the text bolder.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_ero(im, num):\n",
    "    \"\"\" Perform multiple erosion on the image\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    im        :     np.array\n",
    "                    image file\n",
    "    num       :     int\n",
    "                    number of times to apply erosion\n",
    "    \"\"\"\n",
    "    for i in range(num):\n",
    "        im = erosion(im)\n",
    "    return im\n",
    "\n",
    "def date_time_ocr():\n",
    "    imgs = []\n",
    "    # get images for testing \n",
    "    vidcap = cv2.VideoCapture('../XVR_ch5_main_20220214100004_20220214110005.mp4')\n",
    "    success,frame = vidcap.read()\n",
    "    for i in tqdm(range(8000)):\n",
    "        # Capture frame-by-frame\n",
    "        success, frame = vidcap.read()\n",
    "        if not i % 50: \n",
    "            if frame is not None:\n",
    "                imgs.append(frame)\n",
    "            else:\n",
    "                pass\n",
    "    invalid = []\n",
    "    valid = []\n",
    "    datetime_clean = []\n",
    "\n",
    "\n",
    "    for img in tqdm(imgs):\n",
    "        img = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        contrast = 1.5\n",
    "        contrast = max(contrast, 1.0); contrast = min(contrast, 3.0)\n",
    "\n",
    "        brightness = 60\n",
    "        brightness = max(brightness, 0.0); brightness = min(brightness, 100.0)\n",
    "\n",
    "        img = np.clip(contrast * img.astype('float32') \n",
    "                            + brightness, 0.0, 255.0)\n",
    "\n",
    "        img = img.astype('uint8')\n",
    "\n",
    "        img = cv2.adaptiveThreshold(img,\n",
    "                                    255,\n",
    "                                    cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                    cv2.THRESH_BINARY,\n",
    "                                    21, 2)\n",
    "\n",
    "        img = img[coordinates_ocr[0][1]:coordinates_ocr[1][1], \n",
    "                  coordinates_ocr[0][0]:coordinates_ocr[1][0]]\n",
    "\n",
    "        img = multi_ero(img, 2)\n",
    "        datetime_clean.append(img)\n",
    "        text = pytesseract.image_to_string(img, lang='eng', \n",
    "                config='--psm 10 --oem 3 -c tessedit_char_whitelist=0123456789:-')\n",
    "\n",
    "        time_format = r'[0-5]\\d:[0-5]\\d:[0-5]\\d'\n",
    "        date_format = r'\\d{4}-(?:0\\d|1[12])-(?:[0-2]\\d|3[01])'\n",
    "        datetime_format = date_format + time_format\n",
    "        text = text.replace(' ', '')\n",
    "        try:\n",
    "            timestamp_string = re.sub('(\\d{4}-(?:0\\d|1[12])-(?:[0-2]\\d|3[01]))', \n",
    "                                    r'\\1' + r' ', \n",
    "                                    re.findall(datetime_format, text)[0])\n",
    "        except:\n",
    "            invalid.append(text)\n",
    "            continue\n",
    "\n",
    "\n",
    "        if len(text) != 20:\n",
    "            invalid.append(text)\n",
    "\n",
    "        else:\n",
    "            valid.append(text)\n",
    "\n",
    "# def date_time_ocr():            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e854312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2_imshow(datetime_clean[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd5925",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">I used <a href=\"https://pypi.org/project/pytesseract/\">Pytesseract</a> to read the text in the image and convert it to datetime object of python. Pytesseract is an optical character recognition (OCR) tool for python made by Google that uses Deep Learning and in particular LSTM to predict on the text of the image. I used the following configurations:<code>psm</code>=10 so that it it will classify per character, and <code>tessedit_char_whitelist=0123456789:-</code> so that the model would be forced to classify between these whitelist characters which are expected in our date and time element</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ocr_datetime(img, contrast=3, brightness=60):\n",
    "    \"\"\" get the datetime equivalent based on the image\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    img        :    np.array\n",
    "                    image file\n",
    "    contrast   :    int\n",
    "                    contrast between 1-3\n",
    "    brightness :    int \n",
    "                    brightness between 0-100\n",
    "\n",
    "    Returns\n",
    "    =========\n",
    "    get_ocr_datetime  :   datetime.datetime\n",
    "                          datetime equivalent of the cctv image\n",
    "    \"\"\"\n",
    "    # convert to grayscale\n",
    "    img = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    contrast = max(contrast, 1.0)\n",
    "    contrast = min(contrast, 3.0)\n",
    "    \n",
    "    brightness = max(brightness, 0.0) \n",
    "    brightness = min(brightness, 100.0)\n",
    "\n",
    "    # clip image based on contrast and brightness provided\n",
    "    img = np.clip(contrast * img.astype('float32') \n",
    "                        + brightness, 0.0, 255.0)\n",
    "\n",
    "    img = img.astype('uint8')\n",
    "\n",
    "    # perform adaptive thresholding\n",
    "    img = cv2.adaptiveThreshold(img,\n",
    "                              255,\n",
    "                              cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                              cv2.THRESH_BINARY,\n",
    "                              21, 2)\n",
    "\n",
    "    # perform segmentation on the region of interest\n",
    "    img = img[coordinates_ocr[0][1]:coordinates_ocr[1][1], \n",
    "              coordinates_ocr[0][0]:coordinates_ocr[1][0]]\n",
    "\n",
    "    # perform multiple erosion\n",
    "    img = multi_ero(img, 2)\n",
    "    \n",
    "    # get text using pytesseract \n",
    "    text = pytesseract.image_to_string(img, lang='eng', \n",
    "            config='--psm 10 --oem 3 -c tessedit_char_whitelist=0123456789:-')\n",
    "    \n",
    "    # check validity of results\n",
    "    time_format = r'[0-5]\\d:[0-5]\\d:[0-5]\\d'\n",
    "    date_format = r'\\d{4}-(?:0\\d|1[12])-(?:[0-2]\\d|3[01])'\n",
    "    datetime_format = date_format + time_format\n",
    "    text = text.replace(' ', '')\n",
    "\n",
    "    if len(text) == 20:\n",
    "        text = '2022-02-14' + text[10:]\n",
    "    \n",
    "    try:\n",
    "        timestamp_string = re.sub('(\\d{4}-(?:0\\d|1[12])-(?:[0-2]\\d|3[01]))', \n",
    "                                r'\\1' + r' ', \n",
    "                                re.findall(datetime_format, text)[0])\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    return datetime.strptime(timestamp_string, \"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fdddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'correct datetime format percentage: {len(valid)/len(imgs) * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46e46f",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">I checked the format and found out that 70% of the results are correctly classified in a correct datetime format by the model. In particular, it experienced difficulties in classifying number eight when it is near six. However there are about 15 frames per second so there will be a lot of chance for the model to get the correct time. Here is a sample of the output of the pytesseract that is passed to be converted to Python datetime. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36faaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ocr_datetime(imgs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473e609",
   "metadata": {},
   "source": [
    "### b. Datetime information using fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fee20",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">The camera that I am using is a 15 fps which is one of the standard of a CCTV. Using that information, we count the number of frames an employee is sitting on its station then for every 15 frames we count that as 1 second.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec49d306",
   "metadata": {},
   "source": [
    "<h2>6. Integration of timer to workstation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0262df2c",
   "metadata": {},
   "source": [
    "<h3>a. OCR</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7881c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize timer per station\n",
    "# list definition:\n",
    "# list[0] : total time in work station\n",
    "# list[1] : last datetime \n",
    "# list[2] : debt\n",
    "\n",
    "def on_ocr():\n",
    "    timer = {'station_' + str(i): [timedelta(0), None, False] for i in range(1,10)}\n",
    "\n",
    "    # %cd /content/\n",
    "    cap = cv2.VideoCapture('../cctv_footage/Desire_Clip_02.mp4')\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"Total Frame: \",total_frames)\n",
    "\n",
    "    success,frame = cap.read()\n",
    "\n",
    "    width =  1600\n",
    "    height = 900\n",
    "    resize = True\n",
    "    img_array =[]\n",
    "\n",
    "    for i in tqdm(range(total_frames)):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if i <= 1600:\n",
    "            continue\n",
    "\n",
    "        detections, width_ratio, height_ratio = darknet_helper(frame, \n",
    "                                                               width, \n",
    "                                                               height)\n",
    "        detections = non_max_suppression_fast1(detections, 0.65)\n",
    "        detections_bb = []\n",
    "        for label, confidence, bbox in detections:\n",
    "            left, top, right, bottom = bbox2points(bbox)\n",
    "            left, top, right, bottom = (int(left * width_ratio), \n",
    "                                        int(top * height_ratio), \n",
    "                                        int(right * width_ratio), \n",
    "                                        int(bottom * height_ratio))\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), person_color, 2)\n",
    "            cv2.putText(frame, \"{} [{:.2f}]\".format(label, float(confidence)),\n",
    "                                (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                person_color, 4)\n",
    "\n",
    "            detections_bb.append({\n",
    "                    'x1' : left,\n",
    "                    'y1' : top,\n",
    "                    'x2' : right,\n",
    "                    'y2' : bottom\n",
    "                })\n",
    "\n",
    "        thresh = 0.3\n",
    "\n",
    "        for stations, coordinate in coordinates.items():\n",
    "            taken = False\n",
    "            for detection in detections_bb:\n",
    "                iou = get_iou(coordinate, detection)\n",
    "                if iou >= thresh:\n",
    "                    taken = True\n",
    "                    break\n",
    "\n",
    "            if taken or timer[stations][2]:\n",
    "                ocr_time = get_ocr_datetime(frame)\n",
    "                if ocr_time is None:\n",
    "                    timer[stations][2] = True\n",
    "                    continue\n",
    "                else:\n",
    "                    timer[stations][2] = False\n",
    "                    if timer[stations][1] is None:\n",
    "                        timer[stations][1] = ocr_time\n",
    "                    else:\n",
    "                        if timer[stations][1] > ocr_time:\n",
    "                            # invalid time\n",
    "                            timer[stations][2] = True\n",
    "                        elif (ocr_time - timer[stations][1]) <= timedelta(seconds=3):\n",
    "                            timer[stations][0] += (ocr_time - timer[stations][1])\n",
    "                            timer[stations][1] = ocr_time\n",
    "                        else:\n",
    "                            # invalid time\n",
    "                            timer[stations][2] = True\n",
    "\n",
    "            color = taken_color if taken else vacant_color\n",
    "\n",
    "            cv2.rectangle(frame, (coordinate['x1'], coordinate['y1']), \n",
    "                            (coordinate['x2'], coordinate['y2']), color, 2)\n",
    "\n",
    "            cv2.putText(frame, f\"{stations} [{str(timer[stations][0])}]\",\n",
    "                        (coordinate['x1'], coordinate['y1'] - 5), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                        color, 2)\n",
    "\n",
    "        if resize:\n",
    "            frame = cv2.resize(frame, (width, height), \n",
    "                                interpolation=cv2.INTER_AREA)\n",
    "        img_array.append(frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "# on_ocr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8155f41",
   "metadata": {},
   "source": [
    "### b. FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b13273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize timer per station\n",
    "# list definition:\n",
    "# list[0] : total time in work station\n",
    "# list[1] : number of frames in each work station\n",
    "timer = {'station_' + str(i): [timedelta(0), 0] for i in range(1,len(coordinates)+1)}\n",
    "cap = cv2.VideoCapture('../cctv_footage/1.mp4')\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "success,frame = cap.read()\n",
    "\n",
    "width =  frame_width\n",
    "height = frame_height\n",
    "resize = False\n",
    "img_array =[]\n",
    "for i in tqdm(range(total_frames-1)):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "#     if i <= 1600:\n",
    "#         continue\n",
    "\n",
    "    detections, width_ratio, height_ratio = darknet_helper(frame, \n",
    "                                                           width, \n",
    "                                                           height)\n",
    "    detections = non_max_suppression_fast1(detections, 0.65)\n",
    "    detections_bb = []\n",
    "    for label, confidence, bbox in detections:\n",
    "        left, top, right, bottom = bbox2points(bbox)\n",
    "        left, top, right, bottom = (int(left * width_ratio), \n",
    "                                    int(top * height_ratio), \n",
    "                                    int(right * width_ratio), \n",
    "                                    int(bottom * height_ratio))\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), person_color, 2)\n",
    "        \n",
    "        text_size, _ = cv2.getTextSize(\"{} [{:.2f}]\".format(label, float(confidence)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "        cv2.rectangle(frame,\n",
    "              (left, top - text_size[1] - 5),\n",
    "              (left + text_size[0], top - 5),\n",
    "              (0, 0, 0),  # Black color for the background\n",
    "              cv2.FILLED)\n",
    "        \n",
    "        cv2.putText(frame, \"{} [{:.2f}]\".format(label, float(confidence)),\n",
    "                            (left, top - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "                            person_color, 4)\n",
    "        \n",
    "        detections_bb.append({\n",
    "                'x1' : left,\n",
    "                'y1' : top,\n",
    "                'x2' : right,\n",
    "                'y2' : bottom\n",
    "            })\n",
    "    \n",
    "    thresh = 0.2\n",
    "    \n",
    "    for stations, coordinate in coordinates.items():\n",
    "        taken = False\n",
    "        for detection in detections_bb:\n",
    "            iou = get_iou(coordinate, detection)\n",
    "            if iou >= thresh:\n",
    "                taken = True\n",
    "                break\n",
    "        \n",
    "        if taken:\n",
    "            timer[stations][1] += 1\n",
    "            if timer[stations][1] % 10 == 0:\n",
    "                timer[stations][1] = 0\n",
    "                timer[stations][0] += timedelta(seconds=1)\n",
    "              \n",
    "\n",
    "        color = taken_color if taken else vacant_color\n",
    "            \n",
    "        cv2.rectangle(frame, (coordinate['x1'], coordinate['y1']), \n",
    "                        (coordinate['x2'], coordinate['y2']), color, 2)\n",
    "        \n",
    "        text_size, _ = cv2.getTextSize(f\"{stations} [{str(timer[stations][0])}]\", cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "        \n",
    "        cv2.rectangle(frame,\n",
    "              (coordinate['x1'], coordinate['y1'] - text_size[1] - 5),\n",
    "              (coordinate['x1'] + text_size[0], coordinate['y1'] - 5),\n",
    "              (0, 0, 0),  # Black color for the background\n",
    "              cv2.FILLED)\n",
    "        \n",
    "        cv2.putText(frame, f\"{stations} [{str(timer[stations][0])}]\",\n",
    "                    (coordinate['x1'], coordinate['y1'] - 7), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "                    color, 2)\n",
    "        \n",
    "    count_person = len(detections_bb)\n",
    "    cv2.rectangle(frame, (23, 26), \n",
    "                      (208, 63), (0,0,0), -1)\n",
    "    \n",
    "    cv2.putText(frame, f\"Count of Person: {count_person:0>2}\",\n",
    "                    (23 + 5,26+ 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                    (255,255, 255), 2)\n",
    "\n",
    "    if resize:\n",
    "        frame = cv2.resize(frame, (width, height), \n",
    "                            interpolation=cv2.INTER_AREA)\n",
    "#     print(frame)\n",
    "    img_array.append(frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82af7a22",
   "metadata": {},
   "source": [
    "## 7. Save frames as Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a26c3",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">Save frames as video as mp4 and compress it in order to be compatible with Google Colab. The demo video will be submitted separately so it will not blow up the size of the notebook.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ca4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_array_to_video():\n",
    "    fname = '../images/output_video1.mp4'\n",
    "    if not resize:\n",
    "        width= frame_width\n",
    "        height= frame_height\n",
    "\n",
    "    out = cv2.VideoWriter(fname, cv2.VideoWriter_fourcc(*'mp4v'), \n",
    "                          20, (width, height))\n",
    "\n",
    "    for i in tqdm(range(len(img_array))):\n",
    "        frame = cv2.resize(img_array[i], (width, height))\n",
    "        out.write(frame)\n",
    "    out.release()\n",
    "    \n",
    "convert_image_array_to_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a199c1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221cf879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b09ea3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294630a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a6b732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a8513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe903f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d7551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a9e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef45555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caab4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f1f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70635657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d2ac63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c366d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bf83bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b3774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ceebbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da623eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59ac62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec3a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fcada7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096e57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da66da57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f613a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab854cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4543a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
